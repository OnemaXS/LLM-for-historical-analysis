{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad543ea1",
   "metadata": {},
   "source": [
    "# Pipeline for Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2122e0",
   "metadata": {},
   "source": [
    "**TABLE OF CONTENTS**\n",
    "1. Imports\n",
    "2. Auxiliary functions for retrieval and chains\n",
    "3. Retrieval\n",
    "    * a) Loading the embedding for vectorstores\n",
    "    * b) Loading the vectorstores  \n",
    "4. Hyperparameter selection\n",
    "    * a) Hyperparameters\n",
    "    * b) Documenting retrieval\n",
    "    * c) Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a7832f",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14d7cad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Onema/anaconda3/envs/4llama_py11/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/Onema/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token = hf_logging_token)  # UPDATE THE TOKEN FOR RUNNING THE NOTEBOOK\n",
    "#from langchain.chains import RetrievalQA, LLMChain\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = openAI_API_key  # UPDATE THE KEY BEFORE RUNNING THE NOTEBOOK \n",
    "import pickle\n",
    "import time\n",
    "from transformers import AutoModel "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0ff05f",
   "metadata": {},
   "source": [
    "## 2. Auxiliary functions for retrieval and chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc6dd80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary functions for the chain\n",
    "\n",
    "def pretty_metadata(metadata_dict):\n",
    "    return metadata_dict[\"autor\"]+\", \"+metadata_dict[\"source\"]+\", ed. \"+str(metadata_dict[\"edition\"])+\" (\"+str(metadata_dict[\"date\"])+\"), \"+metadata_dict[\"in-text location\"]\n",
    "\n",
    "# NOTE: The following 3 function are esentially variations of the same\n",
    "\n",
    "def format_docs_no_print(docs):  # taken from: https://python.langchain.com/docs/use_cases/question_answering/sources\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def format_docs_and_print(docs):\n",
    "    returned_sources = []\n",
    "    print(\"Retrieved context (metadata are not passed to the model):\\n\")\n",
    "    for doc in docs:\n",
    "        print(doc.page_content+\"\\n\"+\"Metadata: \"+pretty_metadata(doc.metadata)+\"\\n\")\n",
    "        returned_sources.append(doc.page_content)\n",
    "    return \"\\n\\n\".join(returned_sources)\n",
    "\n",
    "def format_docs_for_documentation(docs):\n",
    "    returned_sources = []\n",
    "    for doc in docs:\n",
    "        returned_sources.append(doc.page_content+\"\\n\"+\"Metadata: \"+pretty_metadata(doc.metadata)+\"\\n\")\n",
    "    return \"\\n\\n\".join(returned_sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022078eb",
   "metadata": {},
   "source": [
    "## 3. Retrieval "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c143a4",
   "metadata": {},
   "source": [
    "### a) Loading the embedding for vectorstores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c0d0c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Onema/anaconda3/envs/4llama_py11/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-de', trust_remote_code=True) \n",
    "model_name = \"jinaai/jina-embeddings-v2-base-de\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "hf_jina = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676dded2",
   "metadata": {},
   "source": [
    "### b) Loading the vectorstores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34cf475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"../data/vectorDB\"\n",
    "\n",
    "db_700_without_reg = FAISS.load_local(dir_path+\"/\"+\"faiss_vecDB_700_without_reg\", hf_jina, allow_dangerous_deserialization = True)\n",
    "db_1500_without_reg = FAISS.load_local(dir_path+\"/\"+\"faiss_vecDB_1500_without_reg\", hf_jina, allow_dangerous_deserialization = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a798203",
   "metadata": {},
   "source": [
    "## 5. Hyperparameters selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce9be82",
   "metadata": {},
   "source": [
    "### a) Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f245497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model name\n",
    "model_name = \"gpt-3.5-turbo-0125\"\n",
    "\n",
    "# hyperparameters (3x2x3 = 12 model versions)\n",
    "selected_temps = [0, 0.2, 0.5]\n",
    "vectorstore_ids = [700, 1500]   # strings 700 & 1500 act as ids\n",
    "no_retrieved_chunks = [3, 5, 7]\n",
    "\n",
    "# questions for hyperparameter evaluation\n",
    "questions = [(2, \"Which animals are the most beautiful?\"),\n",
    "            (3, \"Which body parts of birds make them capable of flight?\"),\n",
    "            (5, \"Is there a connection between climate and animal coloration?\")]\n",
    "            # question numbering the same as in the notebook for embedding evaluation\n",
    "\n",
    "# path for documentation\n",
    "save_dir = \"../data/results/hyperparameter_selection\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae7753a",
   "metadata": {},
   "source": [
    "### b) Documenting the retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c750322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare txt files for documenting retrieval\n",
    "for i, question in questions:\n",
    "    for vectorstore_id in vectorstore_ids:\n",
    "        with open(f\"{save_dir}/context_{vectorstore_id}_Q{i}.txt\", \"w\") as f:\n",
    "            f.write(f\"QUESTION {i}: {question} (using db_{vectorstore_id}_without_reg)\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eabc3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter: number of retrieved chunks\n",
    "for n in no_retrieved_chunks:\n",
    "    \n",
    "    # hyperparameter: length of chunks \n",
    "    for vectorstore_id in vectorstore_ids:\n",
    "\n",
    "        if vectorstore_id == 700:\n",
    "            retriever = db_700_without_reg.as_retriever(search_kwargs={\"k\": n})\n",
    "\n",
    "        elif vectorstore_id == 1500:\n",
    "            retriever = db_1500_without_reg.as_retriever(search_kwargs={\"k\": n})\n",
    "            \n",
    "        else:\n",
    "            print(\"Error. Retriever is not specified\")\n",
    "        \n",
    "        for i, question in questions:\n",
    "            \n",
    "            # retrieve           \n",
    "            identified_docs = retriever.get_relevant_documents(question)\n",
    "            \n",
    "            with open(f\"{save_dir}/context_{vectorstore_id}_Q{i}.txt\", \"a\") as f:\n",
    "                f.write(f\"\\n============================================\\nk = {n}\\n\\n\")\n",
    "                f.write(format_docs_for_documentation(identified_docs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef83f987",
   "metadata": {},
   "source": [
    "### c) Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d86467a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the QUESTION based solely on the provided CONTEXT. Explain your answer. If CONTEXT doesn't contain sufficient information to answer the question, say you that you don't know the answer.\n",
    "QUESTION: {question}\n",
    "CONTEXT: {context}\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41b35093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare txt files for documenting inference\n",
    "for i, question in questions:\n",
    "    with open(f\"{save_dir}/generation_Q{i}.txt\", \"w\") as f:\n",
    "            f.write(f\"QUESTION {i}: {question}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2b9f980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed  gpt-3.5-turbo-0125, temperature = 0, chunk size = 700, no. retrieved chunks = 3\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0, chunk size = 700, no. retrieved chunks = 3\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0, chunk size = 700, no. retrieved chunks = 3\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0, chunk size = 1500, no. retrieved chunks = 3\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0, chunk size = 1500, no. retrieved chunks = 3\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0, chunk size = 1500, no. retrieved chunks = 3\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0, chunk size = 700, no. retrieved chunks = 5\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0, chunk size = 700, no. retrieved chunks = 5\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0, chunk size = 700, no. retrieved chunks = 5\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0, chunk size = 1500, no. retrieved chunks = 5\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0, chunk size = 1500, no. retrieved chunks = 5\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0, chunk size = 1500, no. retrieved chunks = 5\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0, chunk size = 700, no. retrieved chunks = 7\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0, chunk size = 700, no. retrieved chunks = 7\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0, chunk size = 700, no. retrieved chunks = 7\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0, chunk size = 1500, no. retrieved chunks = 7\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0, chunk size = 1500, no. retrieved chunks = 7\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0, chunk size = 1500, no. retrieved chunks = 7\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.2, chunk size = 700, no. retrieved chunks = 3\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.2, chunk size = 700, no. retrieved chunks = 3\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.2, chunk size = 700, no. retrieved chunks = 3\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.2, chunk size = 1500, no. retrieved chunks = 3\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.2, chunk size = 1500, no. retrieved chunks = 3\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.2, chunk size = 1500, no. retrieved chunks = 3\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.2, chunk size = 700, no. retrieved chunks = 5\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.2, chunk size = 700, no. retrieved chunks = 5\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.2, chunk size = 700, no. retrieved chunks = 5\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.2, chunk size = 1500, no. retrieved chunks = 5\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.2, chunk size = 1500, no. retrieved chunks = 5\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.2, chunk size = 1500, no. retrieved chunks = 5\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.2, chunk size = 700, no. retrieved chunks = 7\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.2, chunk size = 700, no. retrieved chunks = 7\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.2, chunk size = 700, no. retrieved chunks = 7\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.2, chunk size = 1500, no. retrieved chunks = 7\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.2, chunk size = 1500, no. retrieved chunks = 7\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.2, chunk size = 1500, no. retrieved chunks = 7\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.5, chunk size = 700, no. retrieved chunks = 3\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.5, chunk size = 700, no. retrieved chunks = 3\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.5, chunk size = 700, no. retrieved chunks = 3\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.5, chunk size = 1500, no. retrieved chunks = 3\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.5, chunk size = 1500, no. retrieved chunks = 3\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.5, chunk size = 1500, no. retrieved chunks = 3\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.5, chunk size = 700, no. retrieved chunks = 5\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.5, chunk size = 700, no. retrieved chunks = 5\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.5, chunk size = 700, no. retrieved chunks = 5\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.5, chunk size = 1500, no. retrieved chunks = 5\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.5, chunk size = 1500, no. retrieved chunks = 5\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.5, chunk size = 1500, no. retrieved chunks = 5\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.5, chunk size = 700, no. retrieved chunks = 7\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.5, chunk size = 700, no. retrieved chunks = 7\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.5, chunk size = 700, no. retrieved chunks = 7\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.5, chunk size = 1500, no. retrieved chunks = 7\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.5, chunk size = 1500, no. retrieved chunks = 7\n",
      "Processed  gpt-3.5-turbo-0125, temperature = 0.5, chunk size = 1500, no. retrieved chunks = 7\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter: temperature\n",
    "for temp in selected_temps:\n",
    "    \n",
    "    llm = ChatOpenAI(model_name=model_name, temperature=temp, max_tokens = 400)   # 400 tokens is about 300 words\n",
    "    \n",
    "    # hyperparameter: number of retrieved chunks\n",
    "    for n in no_retrieved_chunks:\n",
    "    \n",
    "        # hyperparameter: length of chunks \n",
    "        for vectorstore_id in vectorstore_ids:\n",
    "\n",
    "            if vectorstore_id == 700:\n",
    "                retriever = db_700_without_reg.as_retriever(search_kwargs={\"k\": n})\n",
    "\n",
    "            elif vectorstore_id == 1500:\n",
    "                retriever = db_1500_without_reg.as_retriever(search_kwargs={\"k\": n})\n",
    "            \n",
    "            else:\n",
    "                print(\"Error. Retriever is not specified\")\n",
    "                \n",
    "            \n",
    "            # RAG chain\n",
    "            chain = (\n",
    "                    {\"context\": retriever | format_docs_no_print, \"question\": RunnablePassthrough()}\n",
    "                    | prompt\n",
    "                    | llm\n",
    "                        )\n",
    "            \n",
    "            # try the model given its hyperparapers for each of the questions\n",
    "            for i, question in questions:\n",
    "                inference = chain.invoke(question)\n",
    "            \n",
    "                # document the result\n",
    "                hp_details = f\"{model_name}, temperature = {temp}, chunk size = {vectorstore_id}, no. retrieved chunks = {n}\"\n",
    "\n",
    "                with open(f\"{save_dir}/generation_Q{i}.txt\", \"a\") as f:\n",
    "                    f.write(hp_details)\n",
    "                    f.write(\"\\n\")\n",
    "                    f.write(inference.pretty_repr())\n",
    "                    f.write(\"\\n\\n\\n\\n\")\n",
    "                \n",
    "                print(\"Processed \", hp_details)\n",
    "                time.sleep(30) # avoid too many requests of the API\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a03d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
